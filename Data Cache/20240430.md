### Messages

***

**Forwarded from [mattimouse](https://t.me/mattim0use)**

checking in here, just updating the fecking buttons for our various pool contracts because wagmi decided to change errthang regard writeAsync which is annoying. but this will help us in the future and hopefully they dont change anything anymore haha

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System; 
using System.Numerics; 
using System.Runtime.InteropServices; 
using SharpDX.Direct3D11; 
using T3.Core.Logging; 
using T3.Core.Resource; 
using Buffer = SharpDX.Direct3D11.Buffer; 
 
namespace T3.Core.Rendering.Material; 
 
/// <summary> 
/// Contains all settings and resource views to initialize a draw call with this material 
/// </summary> 
public class PbrMaterial: IDisposable 
{ 
    public string Name; 
    public ShaderResourceView AlbedoMapSrv; 
    public ShaderResourceView EmissiveMapSrv; 
    public ShaderResourceView RoughnessMetallicOcclusionSrv; 
    public ShaderResourceView NormalSrv; 
 
    public PbrParameters Parameters; 
    public Buffer ParameterBuffer; 
 
    public void UpdateParameterBuffer() 
    { 
        ResourceManager.SetupConstBuffer(Parameters, ref ParameterBuffer); 
    } 
 
    [StructLayout(LayoutKind.Explicit, Size = Stride)] 
    public struct PbrParameters 
    { 
        [FieldOffset(0)] 
        public Vector4 BaseColor; 
 
        [FieldOffset(4 * 4)] 
        public Vector4 EmissiveColor; 
 
        [FieldOffset(8 * 4)] 
        public float Roughness; 
 
        [FieldOffset(9 * 4)] 
        public float Specular; 
 
        [FieldOffset(10 * 4)] 
        public float Metal; 
 
        [FieldOffset(11 * 4)] 
        private float __padding; 
 
        public const int Stride = 12 * 4; 
    } 
 
 
    public static PbrMaterial CreateDefault() 
    { 
        _defaultRmoTexture = TextureUtils.CreateColorTexture(new Vector4(0f, 0, 1, 0)); 
        _defaultNormalTexture = TextureUtils.CreateColorTexture(new Vector4(0.5f, 0.5f, 1, 0)); 
 
        BlackPixelSrv = new ShaderResourceView(ResourceManager.Device, PbrContextSettings.BlackPixelTexture); 
         
        WhitePixelSrv = new ShaderResourceView(ResourceManager.Device, PbrContextSettings.WhitePixelTexture); 
        DefaultAlbedoColorSrv = WhitePixelSrv; 
         
        DefaultEmissiveColorSrv = new ShaderResourceView(ResourceManager.Device, PbrContextSettings.WhitePixelTexture); 
        DefaultRoughnessMetallicOcclusionSrv = new ShaderResourceView(ResourceManager.Device, _defaultRmoTexture); 
        DefaultNormalSrv = new ShaderResourceView(ResourceManager.Device, _defaultNormalTexture); 
         
        var newMaterial= new PbrMaterial() 
                             { 
                                 Name = "Default", 
                                 Parameters = _defaultParameters, 
                                 AlbedoMapSrv = DefaultAlbedoColorSrv, 
                                 EmissiveMapSrv = DefaultEmissiveColorSrv, 
                                 RoughnessMetallicOcclusionSrv = DefaultRoughnessMetallicOcclusionSrv, 
                                 NormalSrv = DefaultNormalSrv, 
 
                             }; 
        newMaterial.UpdateParameterBuffer(); 
        return newMaterial; 
    } 
 
    public static ShaderResourceView BlackPixelSrv { get; set; } 
 
    private static readonly PbrParameters _defaultParameters = new() 
                                                                   { 
                                                                       BaseColor = Vector4.One, 
                                                                       EmissiveColor = new Vector4(0, 0, 0, 1), 
                                                                       Roughness = 0.5f, 
                                                                       Specular = 10, 
                                                                       Metal = 0 
                                                                   }; 
    public static ShaderResourceView DefaultEmissiveColorSrv; 
    public static ShaderResourceView DefaultAlbedoColorSrv; 
    public static ShaderResourceView WhitePixelSrv; 
    public static ShaderResourceView DefaultRoughnessMetallicOcclusionSrv; 
    public static ShaderResourceView DefaultNormalSrv; 
     
    public static Texture2D _defaultRmoTexture; 
    public static Texture2D _defaultNormalTexture;

***

**Forwarded from [ivcained](https://t.me/ivcained)**

All;
                    _disabledBlendState = new BlendState(ResourceManager.Device, blendStateDescription);
                }

                return _disabledBlendState;
            }
        }

        public static Color4 DefaultBlendFactor { get { return new Color4(1, 1, 1, 1); } }

        private static RasterizerState _defaultRasterizerState;

        public static RasterizerState DefaultRasterizerState
        {
            get
            {
                if (_defaultRasterizerState == null && ResourceManager.Device != null)
                {
                    var desc = new RasterizerStateDescription
                                   {
                                       FillMode = FillMode.Solid,
                                       CullMode = CullMode.Back,
                                       IsDepthClipEnabled = true
                                   };
                    _defaultRasterizerState = new RasterizerState(ResourceManager.Device, desc);
                }

                return _defaultRasterizerState;
            }
        }

        private static SamplerState _defaultSamplerState;

        public static SamplerState DefaultSamplerState
        {
            get
            {
                if (_defaultSamplerState == null && ResourceManager.Device != null)
                {
                    var desc = new SamplerStateDescription
                                   {
                                       Filter = Filter.MinMagMipPoint,
                                       AddressU = TextureAddressMode.Clamp,
                                       AddressV = TextureAddressMode.Clamp,
                                       AddressW = TextureAddressMode.Clamp,
                                       MaximumAnisotropy = 16,
                                       ComparisonFunction = Comparison.Never,
                                       MaximumLod = Single.MaxValue
                                   };

                    _defaultSamplerState = new SamplerState(ResourceManager.Device, desc);
                }

                return _defaultSamplerState;
            }
        }
    }
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System;
using SharpDX;
using SharpDX.Direct3D11;
using T3.Core.Resource;

namespace T3.Core.Rendering
{
    /// <summary>
    /// A helper class to generate and provide reusable default states for various DX rendering states
    /// </summary>
    public static class DefaultRenderingStates
    {
        private static DepthStencilState _defaultDepthStencilState;

        public static DepthStencilState DefaultDepthStencilState
        {
            get
            {
                if (_defaultDepthStencilState == null && ResourceManager.Device != null)
                {
                    var depthStencilDescription = new DepthStencilStateDescription
                                                      {
                                                          IsDepthEnabled = true,
                                                          DepthWriteMask = DepthWriteMask.All,
                                                          DepthComparison = Comparison.Less,
                                                          StencilReadMask = 255,
                                                          StencilWriteMask = 255
                                                      };
                    _defaultDepthStencilState = new DepthStencilState(ResourceManager.Device, depthStencilDescription);
                }

                return _defaultDepthStencilState;
            }
        }

        private static DepthStencilState _disabledDepthStencilState;

        public static DepthStencilState DisabledDepthStencilState
        {
            get
            {
                if (_disabledDepthStencilState == null && ResourceManager.Device != null)
                {
                    var depthStencilDescription = new DepthStencilStateDescription
                                                      {
                                                          IsDepthEnabled = false
                                                      };
                    _disabledDepthStencilState = new DepthStencilState(ResourceManager.Device, depthStencilDescription);
                }

                return _disabledDepthStencilState;
            }
        }

        private static BlendState _defaultBlendState;

        public static BlendState DefaultBlendState
        {
            get
            {
                if (_defaultBlendState == null && ResourceManager.Device != null)
                {
                    var blendStateDescription = new BlendStateDescription();
                    blendStateDescription.RenderTarget[0].IsBlendEnabled = true;
                    blendStateDescription.RenderTarget[0].SourceBlend = BlendOption.SourceAlpha;
                    blendStateDescription.RenderTarget[0].DestinationBlend = BlendOption.InverseSourceAlpha;
                    blendStateDescription.RenderTarget[0].BlendOperation = BlendOperation.Add;
                    blendStateDescription.RenderTarget[0].SourceAlphaBlend = BlendOption.One;
                    blendStateDescription.RenderTarget[0].DestinationAlphaBlend = BlendOption.InverseSourceAlpha;
                    blendStateDescription.RenderTarget[0].AlphaBlendOperation = BlendOperation.Add;
                    blendStateDescription.RenderTarget[0].RenderTargetWriteMask = ColorWriteMaskFlags.All;
                    blendStateDescription.AlphaToCoverageEnable = false;
                    _defaultBlendState = new BlendState(ResourceManager.Device, blendStateDescription);
                }

                return _defaultBlendState;
            }
        }

        private static BlendState _disabledBlendState;

        public static BlendState DisabledBlendState
        {
            get
            {
                if (_disabledBlendState == null && ResourceManager.Device != null)
                {
                    var blendStateDescription = new BlendStateDescription();
                    blendStateDescription.RenderTarget[0].IsBlendEnabled = false;
                    blendStateDescription.RenderTarget[0].RenderTargetWriteMask = ColorWriteMaskFlags.

***

**Forwarded from [ivcained](https://t.me/ivcained)**

public void Dispose() => Dispose(true); 
 
    protected virtual void Dispose(bool disposing) 
    { 
        Log.Debug($"Disposing PbrMaterial {Name}..."); 
        AlbedoMapSrv?.Dispose(); 
        EmissiveMapSrv?.Dispose(); 
        RoughnessMetallicOcclusionSrv?.Dispose(); 
        NormalSrv?.Dispose(); 
        ParameterBuffer?.Dispose(); 
    } 
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System.Numerics;

namespace T3.Core.Rendering
{
    public interface ICameraPropertiesProvider
    {
        public Matrix4x4 CameraToClipSpace { get;  set; }
        public Matrix4x4 WorldToCamera { get; set; }
        public Matrix4x4 LastObjectToWorld { get; set; }
    }
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

Render.

***

**Forwarded from [ivcained](https://t.me/ivcained)**

namespace T3.Core.Animation
{
    public static class Utils
    {
        public enum OutsideCurveBehavior
        {
            Constant = 0,
            Cycle,
            CycleWithOffset,
            Oscillate
        };

        static public IOutsideCurveMapper CreateOutsideCurveMapper(OutsideCurveBehavior outsideBehavior)
        {
            switch (outsideBehavior)
            {
                case OutsideCurveBehavior.Constant: return new ConstantCurveMapper();
                case OutsideCurveBehavior.Cycle: return new CycleCurveMapper();
                case OutsideCurveBehavior.CycleWithOffset: return new CycleWithOffsetCurveMapper();
                case OutsideCurveBehavior.Oscillate: return new OscillateCurveMapper();
            }
            throw new System.Exception("undefined outside behavior");
        }

        //internal static void AddKeyframeAtTime(ICurve curve, double time, double value)
        //{
        //    var newKey = new VDefinition();

        //    double? prevU = curve.GetPreviousU(time);
        //    if (prevU != null)
        //        newKey = curve.GetV(prevU.Value).Clone();

        //    newKey.Value = value;

        //    curve.AddOrUpdateV(time, newKey);
        //}
    }
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System.Collections.Generic;

namespace T3.Core.Animation
{
    public interface IOutsideCurveMapper
    {
        void Calc(double u, SortedList<double, VDefinition> curveElements, out double newU, out double offset);
    };
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System.Collections.Generic;
using System.Linq;

namespace T3.Core.Animation
{
    public class CycleWithOffsetCurveMapper : IOutsideCurveMapper
    {
        public void Calc(double u, SortedList<double, VDefinition> curveElements, out double newU, out double offset)
        {
            offset = 0.0;
            if (curveElements.Count < 2)
            {
                newU = u;
            }
            else
            {
                var first = curveElements.First();
                var last = curveElements.Last();
                double firstU = first.Key;
                double lastU = last.Key;
                double delta = 0.0;
                double off = last.Value.Value - first.Value.Value;

                if (u < firstU)
                {
                    delta = firstU - u;
                    newU = lastU - (delta % (lastU - firstU));
                    offset = off * (-((int)(delta / (lastU - firstU)) + 1));
                }
                else if (u > lastU)
                {
                    delta = u - lastU;
                    newU = firstU + (delta % (lastU - firstU));
                    offset = off * ((int)(delta / (lastU - firstU)) + 1);
                }
                else
                {
                    newU = u;
                }
            }
        }
    };
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System.Collections.Generic;
using System.Linq;

namespace T3.Core.Animation
{
    public class CycleCurveMapper : IOutsideCurveMapper
    {
        public void Calc(double u, SortedList<double, VDefinition> curveElements, out double newU, out double offset)
        {
            offset = 0.0;
            if (curveElements.Count < 2)
            {
                newU = u;
            }
            else
            {
                double firstU = curveElements.First().Key;
                double lastU = curveElements.Last().Key;
                double delta = 0.0;

                if (u < firstU)
                {
                    delta = firstU - u;
                    newU = lastU - (delta % (lastU - firstU));
                }
                else if (u > lastU)
                {
                    delta = u - lastU;
                    newU = firstU + (delta % (lastU - firstU));
                }
                else
                {
                    newU = u;
                }
            }
        }
    };

}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System.Collections.Generic;
using System.Linq;

namespace T3.Core.Animation
{
    public class CycleCurveMapper : IOutsideCurveMapper
    {
        public void Calc(double u, SortedList<double, VDefinition> curveElements, out double newU, out double offset)
        {
            offset = 0.0;
            if (curveElements.Count < 2)
            {
                newU = u;
            }
            else
            {
                double firstU = curveElements.First().Key;
                double lastU = curveElements.Last().Key;
                double delta = 0.0;

                if (u < firstU)
                {
                    delta = firstU - u;
                    newU = lastU - (delta % (lastU - firstU));
                }
                else if (u > lastU)
                {
                    delta = u - lastU;
                    newU = firstU + (delta % (lastU - firstU));
                }
                else
                {
                    newU = u;
                }
            }
        }
    };

}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System;
using System.Collections.Generic;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using T3.Core.DataTypes;
using T3.Core.Resource;

namespace T3.Core.Animation
{
    public class CurveState
    {
        public SortedList<double, VDefinition> Table { get; set; }

        public Utils.OutsideCurveBehavior PreCurveMapping
        {
            get => _preCurveMapping;
            set
            {
                _preCurveMapping = value;
                PreCurveMapper = Utils.CreateOutsideCurveMapper(value);
            }
        }
        public Utils.OutsideCurveBehavior PostCurveMapping
        {
            get => _postCurveMapping;
            set
            {
                _postCurveMapping = value;
                PostCurveMapper = Utils.CreateOutsideCurveMapper(value);
            }
        }

        public IOutsideCurveMapper PreCurveMapper { get; private set; }
        public IOutsideCurveMapper PostCurveMapper { get; private set; }

        public CurveState()
        {
            Table = new SortedList<double, VDefinition>();
            PreCurveMapping = Utils.OutsideCurveBehavior.Constant;
            PostCurveMapping = Utils.OutsideCurveBehavior.Constant;
        }

        public CurveState Clone()
        {
            var clone = new CurveState {PreCurveMapping = _preCurveMapping, PostCurveMapping = _postCurveMapping};

            foreach (var point in Table)
                clone.Table[point.Key] = point.Value.Clone();

            return clone;
        }

        public virtual void Write(JsonTextWriter writer)
        {
            writer.WritePropertyName("Curve");
            writer.WriteStartObject();

            writer.WriteObject("PreCurve", PreCurveMapping);
            writer.WriteObject("PostCurve", PostCurveMapping);

            // write keys
            writer.WritePropertyName("Keys");
            writer.WriteStartArray();

            foreach (var point in Table)
            {
                writer.WriteStartObject();

                writer.WriteValue("Time", point.Key);
                point.Value.Write(writer);

                writer.WriteEndObject();
            }

            writer.WriteEndArray();

            writer.WriteEndObject();
        }

        public virtual void Read(JToken inputToken)
        {
            JToken curveToken = inputToken["Curve"];
            if (curveToken == null)
                return;

            PreCurveMapping = (Utils.OutsideCurveBehavior)Enum.Parse(typeof(Utils.OutsideCurveBehavior), curveToken["PreCurve"].Value<string>());
            PostCurveMapping = (Utils.OutsideCurveBehavior)Enum.Parse(typeof(Utils.OutsideCurveBehavior), curveToken["PostCurve"].Value<string>());

            foreach (var keyEntry in (JArray) curveToken["Keys"])
            {
                var time = keyEntry["Time"].Value<double>();
                time = Math.Round(time, Curve.TIME_PRECISION);
                var key = new VDefinition();
                key.Read(keyEntry);
                key.U = time;
                Table.Add(time, key);
            }
        }

        private Utils.OutsideCurveBehavior _preCurveMapping;
        private Utils.OutsideCurveBehavior _postCurveMapping;
    }
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

}

                _positionsWithImpact.RemoveAt(_leastImpactedPositionIndex);

                // Resample section of curve...
                ResampleCurve(IndexFromU(_positionsWithImpact[startSampleIndex].Position),
                                IndexFromU(_positionsWithImpact[endSampleIndex].Position));
            }
        }


        class PositionWithImpact
        {
            public double Position;
            public double Impact;
        }


        private int FindIndexWithMinimumImpact()
        {
            var minImpact = double.PositiveInfinity;
            var minImpactIndex = 0;
            var index = -1;
            foreach (var p in _positionsWithImpact)
            {
                index++;
                if (!(p.Impact < minImpact))
                    continue;

                minImpact = p.Impact;
                minImpactIndex = index;
            }
            return minImpactIndex;
        }


        /**
         * Go through the current list of positions and update if necessary.
         * Invalidated positions are marked with an impact of double.NaN.
         */
        public void UpdatePositionImpacts()
        {
            if (_positionsWithImpact.Count <= 2)
                return;

            double u, nextU, previousU;

            // Yes, these are 3 nested loops. Don't ask.
            for (var keyIndex = 1; keyIndex < _positionsWithImpact.Count - 1; ++keyIndex)
            {
                var p = _positionsWithImpact[keyIndex];
                u = p.Position;
                nextU = _positionsWithImpact[keyIndex + 1].Position;
                previousU = _positionsWithImpact[keyIndex - 1].Position;
                var accumulatedImpact = 0.0;


                if (!double.IsNaN(p.Impact))
                    return;

                for (var curveIndex = 0; curveIndex < _curves.Count; ++curveIndex)
                {
                    var curve = _curves[curveIndex];
                    var vDef = curve.GetV(u);
                    if (vDef == null)
                        continue;

                    curve.RemoveKeyframeAt(u);

                    var startSampleIndex = IndexFromU(previousU);
                    var endSampleIndex = IndexFromU(nextU) + 1;

                    for (var sampleIndex = startSampleIndex; sampleIndex <= endSampleIndex && sampleIndex < _sampleCount; ++sampleIndex)
                    {
                        var sampleU = _sampleStartPosition + sampleIndex * SAMPLE_RATE;
                        var originalValue = _samplesForCurves[curveIndex, sampleIndex];
                        var newValue = curve.GetSampledValue(sampleU);
                        accumulatedImpact += Math.Abs(originalValue - newValue);
                    }
                    curve.AddOrUpdateV(u, vDef);
                }
                p.Impact = accumulatedImpact;
            }
        }

        private int IndexFromU(double u)
        {
            var samplePosition = (SAMPLE_RATE * (int)(u / SAMPLE_RATE));
            return (int)((samplePosition - _sampleStartPosition) / SAMPLE_RATE);
        }

        private double _sampleStartPosition;
        private int _sampleCount;
        private List<Curve> _curves;
        private int _maxPositionCount;

        private List<PositionWithImpact> _positionsWithImpact = new();
        const double SAMPLE_RATE = 1 / 60.0;
        private double[,] _samplesForCurves;

        private int _leastImpactedPositionIndex = 0;
    }

}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System;
using System.Collections.Generic;
using System.Linq;
using T3.Core.DataTypes;

namespace T3.Core.Animation
{
    public class CurveOptimizer
    {
        public CurveOptimizer(List<Curve> curves)
        {
            _curves = curves;

            InitializePositions();
            InitializeCurveSampleBuffers();
            ResampleCurve(0, _positionsWithImpact.Count - 1);
        }

        private void InitializePositions()
        {
            // Gather all positions from the curves in a dictionary...
            var uniquePositions = new HashSet<double>();

            foreach (var k in _curves.SelectMany(curve => curve.GetPointTable()))
            {
                uniquePositions.Add(k.Key);
            }

            _positionsWithImpact = new List<PositionWithImpact>(); ;
            foreach (var position in uniquePositions.OrderBy(o => o))
            {
                _positionsWithImpact.Add(new PositionWithImpact() { Position = position, Impact = double.NaN });
            }
        }


        /**
         * Sampling the curve is expensive. So we sample the state before the next optimization
         * for faster comparision. These curves are stored for each curve in an array with the 
         * necessary size for the samplerate between min and max postiion.
         * 
         * After removing keys these sections of the curve have to resampled.
         */
        private void InitializeCurveSampleBuffers()
        {
            _sampleStartPosition = _positionsWithImpact.First().Position;
            _sampleCount = (int)(Math.Abs(_positionsWithImpact.Last().Position - _sampleStartPosition) / SAMPLE_RATE);
            if (_sampleCount < 2)
                return;

            _samplesForCurves = new double[_curves.Count, _sampleCount + 1];

        }

        public void ResampleCurve(int startIndex, int endIndex)
        {
            for (var curveIndex = 0; curveIndex < _curves.Count; ++curveIndex)
            {
                var curve = _curves[curveIndex];
                for (var sampleIndex = startIndex; sampleIndex < _sampleCount && sampleIndex <= endIndex; ++sampleIndex)
                {
                    var samplePos = _sampleStartPosition + sampleIndex * SAMPLE_RATE;
                    _samplesForCurves[curveIndex, sampleIndex] = curve.GetSampledValue(samplePos);
                }
            }
        }


        public void OptimizeCurves(int maxPositionCount)
        {
            _maxPositionCount = maxPositionCount;

            while (_positionsWithImpact.Count > _maxPositionCount)
            {
                UpdatePositionImpacts();

                _leastImpactedPositionIndex = FindIndexWithMinimumImpact();
                var minImpactPosition = _positionsWithImpact[_leastImpactedPositionIndex].Position;

                // Remove keys from curve
                foreach (var curve in _curves)
                {
                    if (curve.HasVAt(minImpactPosition))
                        curve.RemoveKeyframeAt(minImpactPosition);
                }

                // Remove position from list and invalidate neighbours
                var startSampleIndex = 0;
                var endSampleIndex = 0;

                if (_leastImpactedPositionIndex > 0)
                {
                    _positionsWithImpact[_leastImpactedPositionIndex - 1].Impact = double.NaN;
                    startSampleIndex = _leastImpactedPositionIndex - 1;
                }
                if (_leastImpactedPositionIndex > 1)
                {
                    startSampleIndex = _leastImpactedPositionIndex - 2;
                }

                if (_leastImpactedPositionIndex + 2 < _positionsWithImpact.Count)
                {
                    _positionsWithImpact[_leastImpactedPositionIndex + 1].Impact = double.NaN;
                    endSampleIndex = _leastImpactedPositionIndex + 1;
                }

                if (_leastImpactedPositionIndex + 3 < _positionsWithImpact.Count)
                {
                    endSampleIndex = _leastImpactedPositionIndex + 2;

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System.Collections.Generic;

namespace T3.Core.Animation
{
    public class ConstantCurveMapper : IOutsideCurveMapper
    {
        public void Calc(double u, SortedList<double, VDefinition> curveElements, out double newU, out double offset)
        {
            newU = u;
            offset = 0.0;
        }
    };
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System.Collections.Generic;

namespace T3.Core.Animation
{
    public static class ConstInterpolator
    {
        public static void UpdateTangents(List<KeyValuePair<double, VDefinition>> curveElements) { }

        public static double Interpolate(KeyValuePair<double, VDefinition> a, KeyValuePair<double, VDefinition> b, double u)
        {
            return a.Value.Value;
        }
    };

}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System;
using System.Collections.Generic;
using System.Linq;
using ManagedBass;
using ManagedBass.Wasapi;
using T3.Core.Animation;
using T3.Core.Logging;
using T3.Core.Operator;

namespace T3.Core.Audio
{
    /// <summary>
    /// Uses the windows Wasapi audio API to get audio reaction from devices like speakers and microphones
    /// </summary>
    public static class WasapiAudioInput
    {
        /// <summary>
        /// Needs to be called once a frame
        /// Switches input device required
        /// </summary>
        public static void StartFrame(PlaybackSettings settings)
        {
            _fftUpdatesSinceLastFrame = 0;
            
            if (settings == null)
                return;
                    
            if (settings.AudioSource != PlaybackSettings.AudioSources.ExternalDevice)
            {
                if (!string.IsNullOrEmpty(ActiveInputDeviceName))
                {
                    Stop();
                }
                return ;
            }

            var deviceName = settings.AudioInputDeviceName;
            if (ActiveInputDeviceName == deviceName)
                return;
            
            if (string.IsNullOrEmpty(deviceName))
            {
                if (_complainedOnce)
                    return ;
                
                Log.Warning("Can't switch to WASAPI device without a name");
                _complainedOnce = true;
                return ;
            }


            var device = InputDevices.FirstOrDefault(d => d.DeviceInfo.Name == deviceName);
            if (device == null)
            {
                Log.Warning($"Can't find input device {deviceName}");
                _complainedOnce = true;
                return ;
            }

            StartInputCapture(device);
            _complainedOnce = false;
        }


        public static List<WasapiInputDevice> InputDevices
        {
            get
            {
                if (_inputDevices == null)
                    InitializeInputDeviceList();

                return _inputDevices;
            }
        }



        /// <summary>
        /// If device is null we will attempt default input index
        /// </summary>
        private static void StartInputCapture(WasapiInputDevice device)
            //public static void StartInputCapture(string deviceName)
        {
            int inputDeviceIndex = BassWasapi.DefaultInputDevice;

            if (device == null)
            {
                if (_inputDevices.Count == 0)
                {
                    Log.Error("No wasapi input devices found");
                    return;
                }

                Log.Error($"Attempting default input {BassWasapi.DefaultInputDevice}.");
                device = _inputDevices[0];
            }
            else
            {
                Log.Debug($"Initializing WASAPI audio input for  {device.DeviceInfo.Name}... ");
                inputDeviceIndex = device.WasapiDeviceIndex;
            }

            Bass.Configure(Configuration.UpdateThreads, false);
            // Bass.Configure(Configuration.DeviceBufferLength, 1024);

            BassWasapi.Stop();
            BassWasapi.Free();
            if (!BassWasapi.Init(inputDeviceIndex,
                                 Frequency: 0,
                                 Channels: 0,
                                 //Flags: WasapiInitFlags.Buffer | WasapiInitFlags.Exclusive,
                                 Flags: WasapiInitFlags.Buffer,
                                 Buffer: (float)device.DeviceInfo.DefaultUpdatePeriod,
                                 Period: (float)device.DeviceInfo.DefaultUpdatePeriod,
                                 Procedure: _wasapiProcedure,
                                 User: IntPtr.Zero))
            {
                Log.Error("Can't initialize WASAPI:" + Bass.LastError);
                return;
            }

            ActiveInputDeviceName = device.DeviceInfo.Name;
            var result = BassWasapi.Start();
            //Log.Debug("Wasapi.StartInputCapture() -> BassWasapi.

***

**Forwarded from [ivcained](https://t.me/ivcained)**

Start():" + result);
        }
        
        private static void Stop()
        {
            //Log.Debug("Wasapi.Stop()");
            BassWasapi.Stop();
            BassWasapi.Free();
            ActiveInputDeviceName = null;
        }

        private static bool _complainedOnce;

        
        private static void InitializeInputDeviceList()
        {
            _inputDevices = new List<WasapiInputDevice>();

            // Keep in local variable to avoid double evaluation
            var deviceCount = BassWasapi.DeviceCount;

            for (var deviceIndex = 0; deviceIndex < deviceCount; deviceIndex++)
            {
                var deviceInfo = BassWasapi.GetDeviceInfo(deviceIndex);
                var isValidInputDevice = deviceInfo.IsEnabled && (deviceInfo.IsLoopback || deviceInfo.IsInput);

                if (!isValidInputDevice)
                    continue;

                Log.Debug($"Found Wasapi input ID:{_inputDevices.Count} {deviceInfo.Name} LoopBack:{deviceInfo.IsLoopback} IsInput:{deviceInfo.IsInput} (at {deviceIndex})");
                _inputDevices.Add(new WasapiInputDevice()
                                      {
                                          WasapiDeviceIndex = deviceIndex,
                                          DeviceInfo = deviceInfo,
                                      });
            }
        }

        private static int Process(IntPtr buffer, int length, IntPtr user)
        {
            //Log.Debug("Wasapi.Process() #1");
            var level = BassWasapi.GetLevel();
            if (length < 3000)
                return length;

            _lastUpdateTime = Playback.RunTimeInSecs;

            int resultCode;
            if (_fftUpdatesSinceLastFrame == 0)
            {
                resultCode = BassWasapi.GetData(AudioAnalysis.FftGainBuffer, (int)(AudioAnalysis.BassFlagForFftBufferSize | DataFlags.FFTRemoveDC));
                //Log.Debug("Wasapi.Process() Result code after first update:" +resultCode);
            }
            else
            {
                resultCode = BassWasapi.GetData(_fftIntermediate, (int)(AudioAnalysis.BassFlagForFftBufferSize | DataFlags.FFTRemoveDC));
                //Log.Debug("Wasapi.Process() Result code after another update:" +resultCode);
                if (resultCode >= 0)
                {
                    for (var i = 0; i < AudioAnalysis.FftHalfSize; i++)
                    {
                        AudioAnalysis.FftGainBuffer[i] = MathF.Max(_fftIntermediate[i], AudioAnalysis.FftGainBuffer[i]);
                    }
                }
            }

            if (resultCode < 0)
            {
                Log.Debug($"Can't get Wasapi FFT-Data: {Bass.LastError}");
            }

            _lastAudioLevel = (float)(level * 0.00001);
            _fftUpdatesSinceLastFrame++;
            //Log.Debug($"Process with {length} #{_fftUpdatesSinceLastFrame}  L:{audioLevel:0.0}  DevBufLen:{BassWasapi.Info.BufferLength}");
            return length;
        }

        private static int _fftUpdatesSinceLastFrame;

        public class WasapiInputDevice
        {
            public int WasapiDeviceIndex;
            public WasapiDeviceInfo DeviceInfo;
        }

        private static List<WasapiInputDevice> _inputDevices;
        private static readonly float[] _fftIntermediate = new float[AudioAnalysis.FftHalfSize];
        private static readonly WasapiProcedure _wasapiProcedure = Process;
        private static double _lastUpdateTime;

        public static string ActiveInputDeviceName { get; private set; }
        private static float _lastAudioLevel;
        public static float DecayingAudioLevel => (float)(_lastAudioLevel / Math.Max(1, (Playback.RunTimeInSecs - _lastUpdateTime) * 100));
    }
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

namespace T3.Core.Audio
{
    /// <summary>
    /// A helper class to provides current status details of the beat tapping / and timing offsets.
    /// This is mainly used by the editor to publish results to Operator space.
    /// </summary>
    public static class BeatTimingDetails
    {
        public static float BeatDurationInSecs;
        public static float SlideOffsetInSecs;
        public static float WasTapTriggered;
        public static float WasResyncTriggered;
        public static float Bpm;
        public static float SyncMeasureOffset;
        public static float BeatTime;

        public static float LastPhaseOffset;
        public static float BarSync;
        public static float LastTapBarSync;
    }
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

Make the storage of this transparent

***

**Forwarded from [ivcained](https://t.me/ivcained)**

bandIndex = (int)(octaveNormalized * FrequencyBandCount); 
                        if (bandIndex >= FrequencyBandCount) 
                            bandIndex = NoBandIndex; 
                        break; 
                    } 
                } 
                r[i] = bandIndex; 
            } 
 
            return r; 
        } 
         
        private static readonly int[] _bandIndexForFftBinIndices = InitializeBandsLookupsTable(); 
        private const int NoBandIndex = -1; 
 
        private const int FrequencyBandCount = 32; 
        public static readonly float[] FrequencyBands = new float[FrequencyBandCount]; 
        public static readonly float[] FrequencyBandPeaks = new float[FrequencyBandCount]; 
         
        public static readonly float[] FrequencyBandAttacks = new float[FrequencyBandCount]; 
        public static readonly float[] FrequencyBandAttackPeaks = new float[FrequencyBandCount]; 
         
        /// <summary> 
        /// Result of the fft analysis in gain 
        /// </summary> 
        public static readonly float[] FftGainBuffer = new float[FftHalfSize]; 
         
        /// <summary> 
        /// Result of the fft analysis converted to db and mapped to a normalized range    
        /// </summary> 
        public static readonly float[] FftNormalizedBuffer = new float[FftHalfSize]; 
         
        public const DataFlags BassFlagForFftBufferSize = DataFlags.FFT2048; 
        public const int FftHalfSize = 1024;  
 
    } 
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System; 
using ManagedBass; 
using T3.Core.Animation; 
using T3.Core.Utils; 
 
namespace T3.Core.Audio 
{ 
    /// <summary> 
    /// Analyze audio input from internal soundtrack or from selected Wasapi device. 
    /// Transforms the FFT buffer generated by BASS and generates frequency bands and peaks. 
    /// </summary> 
    public static class AudioAnalysis 
    { 
        public static void CompleteFrame(Playback playback) 
        { 
            // if (playback.Settings is { AudioSource: PlaybackSettings.AudioSources.ExternalDevice }) 
            // { 
            //     WasapiAudioInput.CompleteFrame(); 
            // } 
 
            var gainFactor = playback.Settings?.AudioGainFactor ?? 1; 
            var lastTargetIndex = -1; 
 
            for (var binIndex = 0; binIndex < FftHalfSize; binIndex++) 
            { 
                var gain = FftGainBuffer[binIndex] * gainFactor; 
                var gainDb = gain <= 0.000001f ? float.NegativeInfinity : 20 * MathF.Log10(gain); 
 
                var normalizedValue = MathUtils.RemapAndClamp(gainDb, -80, 0, 0, 1); 
                FftNormalizedBuffer[binIndex] = normalizedValue ; 
                 
                var bandIndex = _bandIndexForFftBinIndices[binIndex]; 
                if (bandIndex == NoBandIndex) 
                    continue; 
                 
                if (bandIndex != lastTargetIndex) 
                { 
                    FrequencyBands[bandIndex] = 0; 
                    lastTargetIndex = bandIndex; 
                } 
                 
                FrequencyBands[bandIndex] = MathF.Max(FrequencyBands[bandIndex], normalizedValue); 
            } 
             
            // Update Peaks 
            for (var bandIndex = 0; bandIndex < FrequencyBandCount; bandIndex++) 
            { 
                var lastPeak = FrequencyBandPeaks[bandIndex]; 
 
                var decayFactor = playback.Settings?.AudioDecayFactor ?? 1; 
                var decayed =  lastPeak * decayFactor; 
                var currentValue = FrequencyBands[bandIndex]; 
                var newPeak = MathF.Max(decayed, currentValue); 
                FrequencyBandPeaks[bandIndex] = newPeak; 
 
                const float attackAmplification = 4; 
                var newAttack = (newPeak - lastPeak).Clamp(0, 1000) * attackAmplification; 
                var lastAttackDecayed = FrequencyBandAttacks[bandIndex] * decayFactor;  
                FrequencyBandAttacks[bandIndex] = MathF.Max(newAttack, lastAttackDecayed); 
                FrequencyBandAttackPeaks[bandIndex] = MathF.Max(FrequencyBandAttackPeaks[bandIndex]*0.995f, FrequencyBandAttacks[bandIndex]); 
            } 
        } 
 
         
        /// <summary> 
        /// To convert the fft-buffer into a logarithmic tonal scale similar to the octaves on a keyboard 
        /// we have to accumulate the fft-values into bins of increasing width. This method generated a look-up 
        /// mapping between fft-value and frequency bin. 
        /// </summary> 
        private static int[] InitializeBandsLookupsTable() 
        { 
            var r = new int[FftHalfSize]; 
            const float lowestBandFrequency = 55;  
            const float highestBandFrequency = 15000; 
                 
            var maxOctave = MathF.Log2(highestBandFrequency / lowestBandFrequency);  
            for (var i = 0; i < FftHalfSize; i++) 
            { 
                var bandIndex = NoBandIndex; 
                var freq = (float)i / FftHalfSize * (48000f / 2f); 
 
                switch (i) 
                { 
                    case 0: 
                        break; 
                     
                    // For low frequency bin we fake a direct mapping to avoid gaps 
                    case < 6: 
                        bandIndex = i - 1; 
                        break; 
                    default: 
                    { 
                        var octave = MathF.Log2(freq / lowestBandFrequency); 
                        var octaveNormalized = octave / maxOctave;

***

**Forwarded from [ivcained](https://t.me/ivcained)**

var flags = PositionFlags.Bytes | PositionFlags.MixerNoRampIn | PositionFlags.Decode;

                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.NoRamp, 1);
                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.Volume, 1);
                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.ReverseDirection, 1);
                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.Frequency, DefaultPlaybackFrequency);
                Bass.ChannelSetPosition(StreamHandle, Math.Max(newStreamPos, 0), flags);
            }

            return newStreamPos;
        }

        public void Disable()
        {
            Bass.StreamFree(StreamHandle);
        }
    }
    
    public class AudioClip
    {
        #region serialized attributes 
        public Guid Id;
        public string FilePath;
        public double StartTime;
        public double EndTime;
        public float Bpm = 120;
        public bool DiscardAfterUse = true;
        public bool IsSoundtrack = false;
        #endregion

        /// <summary>
        /// Is initialized after loading...
        /// </summary>
        public double LengthInSeconds;  

        
        #region serialization
        public static AudioClip FromJson(JToken jToken)
        {
            var idToken = jToken[nameof(Id)];

            var idString = idToken?.Value<string>();
            if (idString == null)
                return null;

            var newAudioClip = new AudioClip
                                   {
                                       Id = Guid.Parse(idString),
                                       FilePath = jToken[nameof(FilePath)]?.Value<string>() ?? String.Empty,
                                       StartTime = jToken[nameof(StartTime)]?.Value<double>() ?? 0,
                                       EndTime = jToken[nameof(EndTime)]?.Value<double>() ?? 0,
                                       Bpm = jToken[nameof(Bpm)]?.Value<float>() ?? 0,
                                       DiscardAfterUse = jToken[nameof(DiscardAfterUse)]?.Value<bool>() ?? true,
                                       IsSoundtrack = jToken[nameof(IsSoundtrack)]?.Value<bool>() ?? true,
                                   };
            
            return newAudioClip;
        }

        public void ToJson(JsonTextWriter writer)
        {
            //writer.WritePropertyName(Id.ToString());
            writer.WriteStartObject();
            {
                writer.WriteValue(nameof(Id), Id);
                writer.WriteValue(nameof(StartTime), StartTime);
                writer.WriteValue(nameof(EndTime), EndTime);
                writer.WriteValue(nameof(Bpm), Bpm);
                writer.WriteValue(nameof(DiscardAfterUse), DiscardAfterUse);
                writer.WriteValue(nameof(IsSoundtrack), IsSoundtrack);
                writer.WriteObject(nameof(FilePath), FilePath);
            }
            writer.WriteEndObject();
        }

        #endregion        
    }
}

***

**Forwarded from [ivcained](https://t.me/ivcained)**

Log.Error($"Failed to initialize audio playback for {clip.FilePath}.");
            }
            var duration = (float)Bass.ChannelBytes2Seconds(streamHandle, bytes);
            
            var stream = new AudioClipStream()
                             {
                                 AudioClip = clip,
                                 StreamHandle = streamHandle,
                                 DefaultPlaybackFrequency = defaultPlaybackFrequency,
                                 Duration = duration,
                             };

            clip.LengthInSeconds = duration;
            return stream;
        }

        private const double AudioSyncingOffset = -2.0 / 60.0;
        private const double AudioTriggerDelayOffset = 2.0 / 60.0;
        private const double RecordSyncingOffset = -1.0 / 60.0;

        /// <summary>
        /// We try to find a compromise between letting bass play the audio clip in the correct playback speed which
        /// eventually will drift away from Tooll's Playback time. If the delta between playback and audio-clip time exceeds
        /// a threshold, we resync.
        /// Frequent resync causes audio glitches.
        /// Too large of a threshold can disrupt syncing and increase latency.
        /// </summary>
        /// <param name="playback"></param>
        public void UpdateTimeLive(Playback playback)
        {
            if (playback.PlaybackSpeed == 0)
            {
                Bass.ChannelPause(StreamHandle);
                return;
            }
            
            var localTargetTimeInSecs = TargetTime - playback.SecondsFromBars(AudioClip.StartTime);
            var isOutOfBounds = localTargetTimeInSecs < 0 || localTargetTimeInSecs >= AudioClip.LengthInSeconds;
            var isPlaying = Bass.ChannelIsActive(StreamHandle) == PlaybackState.Playing;
            
            if (isOutOfBounds)
            {
                if (isPlaying)
                {
                    //Log.Debug("Pausing");
                    Bass.ChannelPause(StreamHandle);
                }
                return;
            }

            if (!isPlaying)
            {
                //Log.Debug("Restarting");
                Bass.ChannelPlay(StreamHandle);
            }

            var currentStreamPos = Bass.ChannelGetPosition(StreamHandle);
            var currentPos = Bass.ChannelBytes2Seconds(StreamHandle, currentStreamPos) - AudioSyncingOffset;
            var soundDelta = (currentPos - localTargetTimeInSecs) * playback.PlaybackSpeed;

            // we may not fall behind or skip ahead in playback
            var maxSoundDelta = ProjectSettings.Config.AudioResyncThreshold * Math.Abs(playback.PlaybackSpeed);
            if (Math.Abs(soundDelta) <= maxSoundDelta)
                return;

            // Resync
            //Log.Debug($"Sound delta {soundDelta:0.000}s for {AudioClip.FilePath}");
            var resyncOffset = AudioTriggerDelayOffset * playback.PlaybackSpeed + AudioSyncingOffset;
            var newStreamPos = Bass.ChannelSeconds2Bytes(StreamHandle, localTargetTimeInSecs + resyncOffset);
            Bass.ChannelSetPosition(StreamHandle, newStreamPos, PositionFlags.Bytes);
        }

        /// <summary>
        /// Update time when recoding, returns number of bytes of the position from the stream start
        /// </summary>
        /// <param name="playback"></param>
        public long UpdateTimeRecord(Playback playback, double fps, bool reinitialize)
        {
            // offset timing dependent on position in clip
            var localTargetTimeInSecs = playback.TimeInSecs - playback.SecondsFromBars(AudioClip.StartTime) + RecordSyncingOffset;
            long newStreamPos = 0;
            if (localTargetTimeInSecs < 0)
                newStreamPos = -Bass.ChannelSeconds2Bytes(StreamHandle, -localTargetTimeInSecs);
            else
                newStreamPos = Bass.ChannelSeconds2Bytes(StreamHandle, localTargetTimeInSecs);

            // re-initialize playback?
            if (reinitialize)
            {

***

**Forwarded from [ivcained](https://t.me/ivcained)**

FftGainBuffer, get256FftValues);
            }
        }

        public static byte[] LastMixDownBuffer(double frameDurationInSeconds)
        {
            if (_clipPlaybacks.Count == 0)
            {
                // get default sample rate
                var channels = clipChannels(null);
                var sampleRate = clipSampleRate(null);
                var samples = (int)Math.Max(Math.Round(frameDurationInSeconds * sampleRate), 0.0);
                var bytes = samples * channels * sizeof(float);

                return new byte[bytes];
            }
            else
            {
                foreach (var (audioClipId, clipStream) in _clipPlaybacks)
                {
                    if (_fifoBuffers.TryGetValue(clipStream.AudioClip, out var buffer))
                    {
                        var bytes = (int)Bass.ChannelSeconds2Bytes(clipStream.StreamHandle, frameDurationInSeconds);

                        var result = buffer.SkipLast(buffer.Length - bytes).ToArray();
                        _fifoBuffers[clipStream.AudioClip] = buffer.Skip(bytes).ToArray();

                        return result;
                    }
                }
            }

            // error
            return null;
        }

        private static double _lastPlaybackSpeed = 1;
        private static bool _bassInitialized;
        private static double _oldBufferInSeconds;
        private static readonly Dictionary<Guid, AudioClipStream> _clipPlaybacks = new();
        private static readonly Dictionary<AudioClip, double> _updatedClipTimes = new();
        private static readonly Dictionary<AudioClip, byte[]> _fifoBuffers = new();

        // to save bass state before recording
        private static int _bassUpdatePeriod; // initial Bass library update period in MS
        private static int _bassGlobalStreamVolume; // initial Bass library sample volume (range 0 to 10000)
        private static int _bassUpdateThreads; // initial Bass library update threads
    }


    public class AudioClipStream
    {
        public AudioClip AudioClip;
        
        public double Duration;
        public int StreamHandle;
        public bool IsInUse;
        public bool IsNew = true;
        public float DefaultPlaybackFrequency { get; private set; }
        public double TargetTime { get; set; }

        public void UpdatePlaybackSpeed(double newSpeed)
        {
            if (newSpeed == 0.0)
            {
                // Stop
                Bass.ChannelStop(StreamHandle);
            }
            else if (newSpeed < 0.0)
            {
                // Play backwards
                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.ReverseDirection, -1);
                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.Frequency, DefaultPlaybackFrequency * -newSpeed);
                Bass.ChannelPlay(StreamHandle);
            }
            else
            {
                // Play forward
                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.ReverseDirection, 1);
                Bass.ChannelSetAttribute(StreamHandle, ChannelAttribute.Frequency, DefaultPlaybackFrequency * newSpeed);
                Bass.ChannelPlay(StreamHandle);
            }
        }


        public static AudioClipStream LoadClip(AudioClip clip)
        {
            if (string.IsNullOrEmpty(clip.FilePath))
                return null;
            
            Log.Debug($"Loading audioClip {clip.FilePath} ...");
            if (!File.Exists(clip.FilePath))
            {
                Log.Error($"AudioClip file '{clip.FilePath}' does not exist.");
                return null;
            }
            var streamHandle = Bass.CreateStream(clip.FilePath, 0, 0, BassFlags.Prescan | BassFlags.Float);
            Bass.ChannelGetAttribute(streamHandle, ChannelAttribute.Frequency, out var defaultPlaybackFrequency);
            Bass.ChannelSetAttribute(streamHandle, ChannelAttribute.Volume, AudioEngine.IsMuted ? 0 : 1);
            var bytes = Bass.ChannelGetLength(streamHandle);
            if (bytes < 0)
            {

***

**Forwarded from [ivcained](https://t.me/ivcained)**

// update our own data
                                        Bass.ChannelUpdate(clipStream.StreamHandle, (int)Math.Round(frameDurationInSeconds * 1000.0));
                                    }

                                    // read all new data that is available
                                    var newBuffer = new byte[bytes];
                                    //Bass.ChannelStop(clipStream.StreamHandle);
                                    //Bass.ChannelPause(clipStream.StreamHandle);
                                    var newBytes = Bass.ChannelGetData(clipStream.StreamHandle, newBuffer, (int)DataFlags.Available);
                                    if (newBytes > 0)
                                    {
                                        newBuffer = new byte[newBytes];
                                        Bass.ChannelGetData(clipStream.StreamHandle, newBuffer, newBytes);
                                        // use number of available bytes to write the data into a new array
                                        //var soundBytesToAdd = Math.Min(newBytes, bytes - buffer.Length);
                                        // append valid data to our previous buffer
                                        //buffer = buffer.Concat(newBuffer.Take(soundBytesToAdd)).ToArray();

                                        buffer = buffer.Concat(newBuffer).ToArray();

                                        // update the FFT now without reading more data
                                        UpdateFftBuffer(clipStream.StreamHandle, playback);
                                    }

                                    // add silence at the end of our buffer if necessary
                                    if (buffer.Length < bytes)
                                    {
                                        var silenceBytesToAdd = bytes - buffer.Length;
                                        var silenceBuffer = new byte[silenceBytesToAdd];
                                        // append data to our previous buffer
                                        buffer = buffer.Concat(silenceBuffer).ToArray();
                                    }
                                }

                                _fifoBuffers[clipStream.AudioClip] = buffer;
                                handledMainSoundtrack = true;
                            }
                        }
                        else
                        {
                            UpdateFftBuffer(clipStream.StreamHandle, playback);
                            clipStream.UpdateTimeLive(playback);
                            handledMainSoundtrack = true;
                        }
                    }
                }
            }

            foreach(var id in obsoleteIds)
            {
                _clipPlaybacks[id].Disable();
                _clipPlaybacks.Remove(id);
            }
            _updatedClipTimes.Clear();
        }

        public static void SetMute(bool configAudioMuted)
        {
            IsMuted = configAudioMuted;
            UpdateMuting();
        }
        
        public static bool IsMuted { get; private set; }
        
        private static void UpdateMuting()
        {
            foreach (var stream in _clipPlaybacks.Values)
            {
                var volume = IsMuted ? 0 : 1;
                Bass.ChannelSetAttribute(stream.StreamHandle,ChannelAttribute.Volume, volume);
            }
        }

        private static void UpdateFftBuffer(int soundStreamHandle, Playback playback)
        {
            int get256FftValues = (int)DataFlags.FFT512;

            // do not advance plaback if we are not in live mode
            if (!playback.IsLive)
                get256FftValues |= (int)268435456; // TODO: find BASS_DATA_NOREMOVE in ManagedBass

            if (playback.Settings != null && playback.Settings.AudioSource == PlaybackSettings.AudioSources.ProjectSoundTrack)
            {
                Bass.ChannelGetData(soundStreamHandle, AudioAnalysis.

***

**Forwarded from [ivcained](https://t.me/ivcained)**

Configure(Configuration.GlobalStreamVolume, _bassGlobalStreamVolume);
            Bass.Configure(Configuration.UpdateThreads, _bassUpdateThreads);
            Bass.Start();
        }

        public static void CompleteFrame(Playback playback,
            double frameDurationInSeconds)
        {
            if (!_bassInitialized)
            {
                Bass.Free();
                Bass.Init();
                _bassInitialized = true;
            }
            AudioAnalysis.CompleteFrame(playback);
            
            // Create new streams
            foreach (var (audioClip, time) in _updatedClipTimes)
            {
                if (_clipPlaybacks.TryGetValue(audioClip.Id, out var clip))
                {
                    clip.TargetTime = time;
                }
                else
                {
                    var audioClipStream = AudioClipStream.LoadClip(audioClip);
                    if (audioClipStream != null)
                        _clipPlaybacks[audioClip.Id] = audioClipStream;
                }
            }
            
            List<Guid> obsoleteIds = new();
            var playbackSpeedChanged = Math.Abs(_lastPlaybackSpeed - playback.PlaybackSpeed) > 0.001f;
            _lastPlaybackSpeed = playback.PlaybackSpeed;

            var handledMainSoundtrack = false;
            foreach (var (audioClipId,clipStream) in _clipPlaybacks)
            {
                clipStream.IsInUse = _updatedClipTimes.ContainsKey(clipStream.AudioClip);
                if (!clipStream.IsInUse)
                {
                    obsoleteIds.Add(audioClipId);
                }
                else
                {
                    if (playback.IsLive && playbackSpeedChanged)
                        clipStream.UpdatePlaybackSpeed(playback.PlaybackSpeed);

                    if (!handledMainSoundtrack && clipStream.AudioClip.IsSoundtrack)
                    {
                        if (!playback.IsLive)
                        {
                            // create buffer if necessary
                            byte[] buffer = null;
                            if (!_fifoBuffers.TryGetValue(clipStream.AudioClip, out buffer))
                                buffer = _fifoBuffers[clipStream.AudioClip] = new byte[0];
                            else
                                buffer = new byte[0];

                            // update time position in clip
                            var streamPositionInBytes = clipStream.UpdateTimeRecord(playback, 1.0 / frameDurationInSeconds, true);

                            var bytes = (int)Math.Max(Bass.ChannelSeconds2Bytes(clipStream.StreamHandle, frameDurationInSeconds), 0);
                            if (buffer != null && bytes > 0)
                            {
                                while (buffer.Length < bytes)
                                {
                                    // add silence at the beginning of our buffer if necessary
                                    if (streamPositionInBytes < 0)
                                    {
                                        // clear the old buffer and replace with silence
                                        _fifoBuffers[clipStream.AudioClip] = new byte[0];
                                        var silenceBytesToAdd = Math.Min(-streamPositionInBytes, bytes);
                                        var silenceBuffer = new byte[silenceBytesToAdd];
                                        // append data to our previous buffer
                                        buffer = buffer.Concat(silenceBuffer).ToArray();
                                    }

                                    if (buffer.Length < bytes)
                                    {
                                        // set the channel buffer size from here on
                                        Bass.ChannelSetAttribute(clipStream.StreamHandle, ChannelAttribute.Buffer,
                                            (int)Math.Round(frameDurationInSeconds * 1000.0));

***

**Forwarded from [ivcained](https://t.me/ivcained)**

using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using ManagedBass;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using SharpDX.Win32;
using T3.Core.Animation;
using T3.Core.IO;
using T3.Core.Logging;
using T3.Core.Operator;
using T3.Core.Resource;

namespace T3.Core.Audio
{
    /// <summary>
    /// Controls loading, playback and discarding of audio clips.
    /// </summary>
    public static class AudioEngine
    {
        public static int clipChannels(AudioClip clip)
        {
            if (clip != null && _clipPlaybacks.TryGetValue(clip.Id, out var stream))
            {
                Bass.ChannelGetInfo(stream.StreamHandle, out var info);
                return info.Channels;
            }

            // by default use stereo
            return 2;
        }
        public static int clipSampleRate(AudioClip clip)
        {
            if (clip != null && _clipPlaybacks.TryGetValue(clip.Id, out var stream))
            {
                Bass.ChannelGetInfo(stream.StreamHandle, out var info);
                return info.Frequency;
            }

            // return default sample rate (48000Hz)
            return 48000;
        }

        public static void UseAudioClip(AudioClip clip, double time)
        {
            _updatedClipTimes[clip] = time;
        }

        public static void ReloadClip(AudioClip clip)
        {
            if (_clipPlaybacks.TryGetValue(clip.Id, out var stream))
            {
                Bass.StreamFree(stream.StreamHandle);
                _clipPlaybacks.Remove(clip.Id);
            }
            
            UseAudioClip(clip,0);
        }

        public static void prepareRecording(Playback playback, double fps)
        {
            _bassUpdateThreads = Bass.GetConfig(Configuration.UpdateThreads);
            _bassUpdatePeriod = Bass.GetConfig(Configuration.UpdatePeriod);
            _bassGlobalStreamVolume = Bass.GetConfig(Configuration.GlobalStreamVolume);

            // turn off automatic sound generation
            Bass.Pause();
            Bass.Configure(Configuration.UpdateThreads, false);
            Bass.Configure(Configuration.UpdatePeriod, 0);
            Bass.Configure(Configuration.GlobalStreamVolume, 0);

            // TODO: Find this in Managed Bass library. It doesn't seem to be present.
            int tailAttribute = (int)16;

            foreach (var (audioClipId, clipStream) in _clipPlaybacks)
            {
                _oldBufferInSeconds = Bass.ChannelGetAttribute(clipStream.StreamHandle, ChannelAttribute.Buffer);

                Bass.ChannelSetAttribute(clipStream.StreamHandle, ChannelAttribute.Volume, 1.0);
                Bass.ChannelSetAttribute(clipStream.StreamHandle, ChannelAttribute.Buffer, 1.0 / fps);
                Bass.ChannelSetAttribute(clipStream.StreamHandle, (ChannelAttribute) tailAttribute, 2.0 / fps);
                Bass.ChannelStop(clipStream.StreamHandle);
                clipStream.UpdateTimeRecord(playback, fps, true);
                Bass.ChannelPlay(clipStream.StreamHandle);
                Bass.ChannelPause(clipStream.StreamHandle);
            }

            _fifoBuffers.Clear();
        }

        public static void endRecording(Playback playback, double fps)
        {
            // TODO: Find this in Managed Bass library. It doesn't seem to be present.
            int tailAttribute = (int)16;

            foreach (var (audioClipId, clipStream) in _clipPlaybacks)
            {
                // Bass.ChannelPause(clipStream.StreamHandle);
                clipStream.UpdateTimeRecord(playback, fps, false);
                Bass.ChannelSetAttribute(clipStream.StreamHandle, ChannelAttribute.NoRamp, 0);
                Bass.ChannelSetAttribute(clipStream.StreamHandle, (ChannelAttribute)tailAttribute, 0.0);
                Bass.ChannelSetAttribute(clipStream.StreamHandle, ChannelAttribute.Buffer, _oldBufferInSeconds);
            }

            // restore live playback values
            Bass.Configure(Configuration.UpdatePeriod, _bassUpdatePeriod);
            Bass.

***

**Forwarded from [ivcained](https://t.me/ivcained)**

And how does it prune data

***

**Forwarded from [ivcained](https://t.me/ivcained)**

How hows this log where?

***

**Forwarded from [ivcained](https://t.me/ivcained)**

Project Naming and Branding Update

We've decided to go with the name "Hack House DAO" (HHD) for our project. This name gives us a lot of flexibility and conveys what we're doing without being too restrictive.
Why HHD Works:
It's simple, easy to remember, and hints at the creative/experimental nature of the project
Doesn't pigeonhole us into a specific area (like dance)
Allows for a universal approach
HHD's Scope: Our vision is to make HHD a parent company with various sub-projects or "houses" focusing on:
Global hackathons
Art exhibitions
Creator support
Community development
Builders
Scholarships
Videos
Charity and more
Sub-Branding Idea: We might brand the initial project as "Base Hack House" or something similar since it has some Base involvement. We can open up further branding participation to the community via Nouns as well.
Other Updates
Sikar reached out wanting to help with the project and get our DAO up and running.
I'll start drafting materials while we await your feedback on the direction with HHD.

Core team: Karsha, Anand, ivc  (multi sig setup on nouns build dao) using gnosis safe. 

Just  wanted to share my recent call with Karsha.

***

**Forwarded from [ivcained](https://t.me/ivcained)**

Based hack house is now a child of the hack house DAO

***

**Forwarded from [Plat0x.eth |  I WILL NEVER DM YOU FOR MONEY](https://t.me/Plat0x_eth)**

If you've participated in past grant rounds, please share any new updates or milestones from the prior months (N/A if you havent participated previously).



Where should we look to see the impact of your project? Please provide links to public websites and/or the latest values for key metrics you track privately. This might be used in future impact reports

https://catts.run

Total Prior Funding Amounts, Sources, and Dates for the project in USD

2024-04-01, Dfinity Developer Grant, $25K

Which of the following funding sources do you have? Check all that apply

Grants

How old (in Months) is the project?

4

Location/Region where the project is based

Europe

Please describe your dApp or App, focusing on its innovative utility or service within the ecosystem. Highlight the problem it solves or the need it addresses.

Composite attestations are a new type of attestation combining data from multiple EAS sources to form a unified and verifiable credential.

Use CATTS to build your digital identity, your reputation, to prove your credentials!

Example: Mint a CATTS attestation that proves that you:

1  Have a Gitcoin Passport score > X

2  Am a RetroPGF 3 grant recipient

3  Have a Coinbase verified wallet

Then use this composite attestation to gain access to some functionality in an app or service.

Describe how your application improves user experience in the Web3 space. Include any specific design choices or features that enhance accessibility and practical utility.

CATTS will radically simplify for projects and communities wanting to consume and use attestations as a way to learn more about their members.

In what ways does your project contribute to financial inclusion, education, or social impact? Explain the specific areas of impact and the target audience or communities you serve.

Combining insights from attestations spread out over multiple addresses on multiple chains allows users to build stronger digital identities and promotes inclusivity.

What type of open source software (OSS) license(s) do you have? Is all of your code available on public repos or is some portion of your codebase private?

MIT, all CATTS source code will be permissibly licensed.

What stage of development is your application currently in, and what are your immediate next steps? Provide a roadmap or development timeline if available.

In active development, first MVP will be demoed by the end of May.

How would funding from this round support your project's development and goals? Outline specific areas where the funds would be allocated (e.g., development, user research, community building).

All funds at the moment goes towards development. At later stages we would need to do some design work and also to bring in other competences: community management, communications etc.

Anything else you'd like to share about your project, previous work, or other project affiliations? Anything you'd like to add that may help in determining project eligibility?

This project is a natural progression from the other projects I have built within the Ethererum Attestation Service space: Praise, Attest Fest, OP Citizens Attestation Explorer.

I am a big believer in attestations as a foundational building block when constructing trust graphs, reputation systems and digital identities.

CATTS will radically simplify this process for projects and users when the knowledge and wisdom that is now spread out over chains and addresses can be consolidated into more coherent and usable credentials.

CATTS have been accepted into the EAS fellowship and is receiving mentorship and support from the core EAS team as well as a from a really solid bunch of mentors.

***

**Forwarded from [Plat0x.eth |  I WILL NEVER DM YOU FOR MONEY](https://t.me/Plat0x_eth)**

About
Composite attestations are a new type of attestation combining data from multiple sources to form a unified and verifiable credential.

The Ethereum Attestation Service (EAS) is an infrastructure public good for making attestations onchain or offchain about anything. Attestations can represent identity, reputation, knowledge, and much more. EAS is a tokenless and free service that is available on mainnet, several L2s, and various testnets. EAS is a great service! It is tokenless and free for anyone to use. This means it is being used. A lot!

There is a universe of attestation data out there. EAS provides an API that allows you to query that data which makes integration into websites and apps easy.

Lets say I, as an app, want to offer membership to users that meet certain criteria. I my backend, I can use the EAS API to query all attestations that are relevant to my use case. Then, I write some custom logic to process the data and let the outcome of that logic determine if a user is eligible for membership. Easy!

But, what if I need to show a proof of the outcome of my logic? What if I need to create an attestation that says This user is eligible for membership? I can of course easily create that attestation. But, without knowledge of the data I processed or about the processing logic I ran on the data, how can anyone verify that the attestation I created is correct?

Wouldnt it be great if there was a way to create attestations based on the result of custom queries and processing logic and have the result of that logic be independently verifiable? Thats where CATTS, the Composite Attestations Engine comes in.

CATTS allows for the creation of composite attestations based on custom queries and processing logic. Running on the Internet Computer (ICP) as a smart contract canister, it leverages data from existing attestations via the EAS GraphQL API, ensuring that the creation and verification of attestations are both reliable and transparent. The processing logic is defined as a piece of arbitrary JavaScript code, which is executed securely within the canister environment. The engine also provides a receipt for each run, detailing the settings used, which aids in verifying the correctness of the composite attestations.

Planned features
Custom queries: Fetch data from the EAS GraphQL API using custom queries.

Custom processing logic: Define custom processing logic to create composite attestations based on the result of the queries.

Secure execution: The processing logic is executed securely within the canister environment.

Receipts: A receipt is created for each run, detailing the settings used, which aids in verifying the correctness of the composite attestations.

Chain agnostic: Run queries on one chain or on multiple chains.

Verifiable: The result of the processing logic is independently verifiable.

Open: The engine is open source and free to use. Anyone can create and run recipes.

Cost effective: Attestation runs can be simulated before they are run. This ensures that the cost of running the canister is minimized.

Future features
Advanced settings:

Allow query chain settings to be overridden on a per run and per query basis.

Allow users to claim their composite attestations:

Transfers cost of creating attestations to the user.

Allow the user to claim an attestation using multiple addresses.
Even more chain agnostic:

Allow the creation of attestations on multiple chains.

Allow querying other attestation services, not just EAS.

ZK attestations:

Allow the creation of zero knowledge attestations.
Additional Information
Project GitHub Repo (Not Team Page):

https://github.com/c-atts/web

Total Team Size

1

List the Github Handles and Wallet Addresses of Each Team Member (e.g. owocki - 0x123, kbw - 0x456):

kristoferlund, 0xa32aECda752cF4EF89956e83d60C04835d4FA867

Link to your Public Group Chat (e.g. Discord or Telegram)

https://t.me/+TTwQJIiT6040YmM0

***

**Forwarded from [RodoTritn ](https://t.me/Rodotriton1)**

Our team's experience in the EVM environment has prepared us for this exciting new chapter. We're eager to leverage our skills and creativity to build groundbreaking projects on L2s this year. This grant opportunity would not only support our development efforts but also encourage us to push the boundaries of what's possible in the blockchain space.


Furthermore, our commitment to decentralized science (DeSci) remains strong. As we venture into L2s, we aim to integrate DeSci principles into our projects, fostering a symbiotic relationship between cutting-edge technology and scientific advancement. Through DeSci, we strive to unlock new possibilities and create a more inclusive and sustainable future for all.

For more information about our projects and missions, please visit the following Gitcoin rounds:

@Fractal_Visions Mission #1 Round: https://explorer.gitcoin.co/#/round/10/5/13

@hypercerts Round: https://explorer.gitcoin.co/#/round/42161/28/20

@tecmns Round: 
https://explorer.gitcoin.co/#/round/10/9/34

@arbitrum's dApps & Apps Round: https://explorer.gitcoin.co/#/round/42161/25/112 

for twitter @Plat0x_eth :'D

***

**Forwarded from [Plat0x.eth |  I WILL NEVER DM YOU FOR MONEY](https://t.me/Plat0x_eth)**

gitcoin update

***

**Forwarded from [Josh](https://t.me/JBate7)**

### Crecimiento
Opening working group call with the Protocol Labs Crecimiento team. Will make good contact with the leader Santi, to let him know that DeSciWorld is available and interested to help with any DeSci aspects to the event

***

**Forwarded from [Josh](https://t.me/JBate7)**

Had a meeting with Eggy from SocialLayer, will be partnering with them for data collection and event listings. DeSciWorld gathers lots of information about events and location of DeSci projects for our own database. We can share that data with Eggy to display on SocialLayer's new event map. The partnership will involve comarketing and displaying our branding wherever we provide data. Potentially we can integrate their maps into our own v2 website

***

**Forwarded from [Josh](https://t.me/JBate7)**

hi

***

**Forwarded from [Plat0x.eth |  I WILL NEVER DM YOU FOR MONEY](https://t.me/Plat0x_eth)**

todo: 
- Cambiar el icono y el texto del header
- Asegurarse que el ano del gato no se mueva con diferentes pantallas
- hacer que el ano del gato se haga mas grande cuando haces hover o mas chiquito cuando haces click
- implementar el modal de staking de fckn 
- arreglar la funcion de apy
-testear funciones
- hacer que el gato de perfil se ponga de frente cuando haces hover en staking
- tooltips que digan que significa (como estan implementadas en fckn)
- cambiar el texto del footer

***

**Forwarded from [Plat0x.eth |  I WILL NEVER DM YOU FOR MONEY](https://t.me/Plat0x_eth)**

Carlo Farms contract todo list -At0x